{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Tuple, List\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Obtener el objeto BeautifulSoup a partir de una URL.\n",
    "\n",
    "    Args:\n",
    "    - url (str): La URL de la página web de la que se va a obtener el objeto\n",
    "    BeautifulSoup.\n",
    "\n",
    "    Returns:\n",
    "    - BeautifulSoup: El objeto BeautifulSoup que representa la estructura de la\n",
    "    página web.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        # Lanza una excepción si hay un error en la solicitud HTTP.\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al obtener la página web: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_csv_from_url(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carga un archivo CSV desde una URL en una tabla de Pandas.\n",
    "\n",
    "    Args:\n",
    "    - url (str): La URL del archivo CSV.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Una tabla de Pandas que contiene los datos del archivo CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        # Lanza una excepción si hay un error en la solicitud HTTP\n",
    "        response.raise_for_status()\n",
    "        s = response.content\n",
    "        return pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al obtener el archivo CSV desde la URL: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def print_tabulate(data_frame: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Imprime una tabla de Pandas.\n",
    "\n",
    "    Args:\n",
    "    - data_frame (pd.DataFrame): La tabla de Pandas que se imprimirá.\n",
    "    \"\"\"\n",
    "    print(tabulate(data_frame, headers=data_frame.columns, tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_nombre_dependencia(nombre_sucio: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el nombre de una dependencia eliminando las dos primeras palabras.\n",
    "\n",
    "    Args:\n",
    "    - nombre_sucio (str): El nombre de la dependencia que se va a limpiar.\n",
    "\n",
    "    Returns:\n",
    "    - str: El nombre de la dependencia limpio.\n",
    "    \"\"\"\n",
    "    return ' '.join(nombre_sucio.split(' ')[2:])\n",
    "\n",
    "def obtener_cantidad_de_filas(data_frame: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Obtiene la cantidad de filas en una tabla de Pandas.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): La tabla de Pandas de la que se va a obtener la\n",
    "    cantidad de filas.\n",
    "\n",
    "    Returns:\n",
    "    - int: La cantidad de filas en la tabla.\n",
    "    \"\"\"\n",
    "    return len(data_frame.index)\n",
    "\n",
    "def limpiar_dato_sueldo(sueldo_txt: str) -> float:\n",
    "    \"\"\"\n",
    "    Convierte un sueldo en formato de cadena de caracteres a un valor numérico\n",
    "    de punto flotante.\n",
    "\n",
    "    Args:\n",
    "    - sueldo_txt (str): El sueldo que se va a limpiar.\n",
    "\n",
    "    Returns:\n",
    "    - float: El sueldo limpio como un valor numérico.\n",
    "    \"\"\"\n",
    "    return float(sueldo_txt[2:].replace(\",\", \"\"))\n",
    "\n",
    "def get_dependencias_uanl() -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Obtiene las dependencias, meses y años disponibles en la página de\n",
    "    transparencia de la UANL.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[List, List[str], List[str]]: Una tupla que contiene la lista de\n",
    "    dependencias, la lista de meses y la lista de años disponibles.\n",
    "    \"\"\"\n",
    "    url = \"http://transparencia.uanl.mx/remuneraciones_mensuales/bxd.php\"\n",
    "    soup = get_soup(url)\n",
    "    table = soup.find_all(\"table\")[0].find_all('tr')\n",
    "    dependencias = [(option['value'], limpiar_nombre_dependencia(option.text))\n",
    "                    for option in table[1].find_all(\"option\")]\n",
    "    meses = [option['value'] for option in table[2].find_all('td')[0]\n",
    "                                                    .find_all(\"option\")]\n",
    "    anios = [option['value'] for option in table[2].find_all('td')[1]\n",
    "                                                    .find_all(\"option\")]\n",
    "    return dependencias, meses, anios\n",
    "\n",
    "def get_pages(periodo: str, area: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Obtiene los enlaces de las páginas disponibles para un período y área\n",
    "    específicos.\n",
    "\n",
    "    Args:\n",
    "    - período (str): El período para el que se desean obtener los enlaces.\n",
    "    - area (str): El área para la que se desean obtener los enlaces.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: Lista de los enlaces disponibles.\n",
    "    \"\"\"\n",
    "    url = \"http://transparencia.uanl.mx/remuneraciones_mensuales/\" \\\n",
    "                + f\"bxd.php?pag_act=1&id_area_form={area}&mya_det={periodo}\"\n",
    "    soup = get_soup(url)\n",
    "    try:\n",
    "        links = soup.find_all(\"table\")[1].find_all('a')\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    return ['1'] + [link.text for link in links]\n",
    "\n",
    "def get_info_transparencia_uanl(periodo: str, area: str,\n",
    "                                    page: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtiene información de transparencia de la UANL para un periodo y área\n",
    "    específicos.\n",
    "\n",
    "    Args:\n",
    "    - periodo (str): El período para el que se desea obtener la información.\n",
    "    - area (str): El área para la que se desea obtener la información.\n",
    "    - page (int): El número de página de la que se desea obtener la información\n",
    "    (opcional, por defecto 1).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Una tabla de Pandas que contiene la información obtenida.\n",
    "    \"\"\"\n",
    "    url = \"http://transparencia.uanl.mx/remuneraciones_mensuales/\" \\\n",
    "            + f\"bxd.php?pag_act={page}&id_area_form={area}&mya_det={periodo}\"\n",
    "    soup = get_soup(url)\n",
    "    table = soup.find_all(\"table\")\n",
    "    try:\n",
    "        table_row = table[2].find_all('tr')\n",
    "        list_of_lists = [[row_column.text.strip()\n",
    "                            for row_column in row.find_all('td')]\n",
    "                            for row in table_row]\n",
    "        data_frame = pd.DataFrame(list_of_lists[1:], columns=list_of_lists[0])\n",
    "        data_frame[\"Sueldo Neto\"] = data_frame[\"Sueldo Neto\"] \\\n",
    "                                        .transform(limpiar_dato_sueldo)\n",
    "        data_frame = data_frame.drop(['Detalle'], axis=1)\n",
    "    except IndexError as e:\n",
    "        print(f\"pagina sin informacion a: {area}, per: {periodo}, page:{page}\")\n",
    "        print(e)\n",
    "        data_frame = pd.DataFrame()\n",
    "    return data_frame\n",
    "\n",
    "def unir_datos(lista_df: List[pd.DataFrame], dep:str, m: str,\n",
    "                    a:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Unir múltiples tablas en una sola, añadiendo información de dependencia,\n",
    "    mes y año.\n",
    "\n",
    "    Args:\n",
    "    - lista_df (List[pd.DataFrame]): Lista de tablas que se van a unir.\n",
    "    - dep (str): La dependencia de la que se va a agrega información.\n",
    "    - m (str): El mes para el que se va a agregar información.\n",
    "    - a (str): El año para el que se va a agregar información.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: La tabla resultante después de unir las demás y agregar\n",
    "    información.\n",
    "    \"\"\"\n",
    "    if len(lista_df) > 0:\n",
    "        data_frame = pd.concat(lista_df)\n",
    "        data_frame[\"dependencia\"] = [dep[1] for i in\n",
    "                            range(0, obtener_cantidad_de_filas(data_frame))]\n",
    "        data_frame[\"mes\"] = [m for i in\n",
    "                            range(0, obtener_cantidad_de_filas(data_frame))]\n",
    "        data_frame[\"anio\"] = [a for i in\n",
    "                            range(0, obtener_cantidad_de_filas(data_frame))]\n",
    "    else:\n",
    "        data_frame= pd.DataFrame()\n",
    "    return data_frame\n",
    "\n",
    "listado_dependencias, listado_meses, listado_anios = get_dependencias_uanl()\n",
    "\n",
    "ldfs = []\n",
    "for anio in listado_anios:\n",
    "    for mes in listado_meses:\n",
    "        for dependencia in listado_dependencias:\n",
    "            pages = get_pages(f\"{mes}{anio}\", dependencia[0])\n",
    "            print(f\"m: {mes} a: {anio} d: {dependencia} p: {pages}\")\n",
    "            ldf = [get_info_transparencia_uanl(f\"{mes}{anio}\", dependencia[0],\n",
    "                    page) for page in pages]\n",
    "            udf = unir_datos(ldf, dependencia, mes, anio)\n",
    "            ldfs.append(udf)\n",
    "\n",
    "df = pd.concat(ldfs)\n",
    "df.to_csv(\"csv/uanl2021.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki() -> pd.DataFrame:\n",
    "    soup = get_soup(\"https://en.wikipedia.org/wiki/List_of_states_of_Mexico\")\n",
    "    table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "\n",
    "    headers = [header.text.strip() for header in table.find_all('th')]\n",
    "    rows = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        columns = [column.text.strip() for column in row.find_all('td')]\n",
    "        rows.append(columns)\n",
    "\n",
    "    return pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "\n",
    "df = wiki()\n",
    "print_tabulate(df)\n",
    "df.to_csv(\"csv/estados.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_number(str_repeated_value: str) -> float:\n",
    "    \"\"\"\n",
    "    Elimina números repetidos y convierte la cadena resultante en un valor\n",
    "    numérico de punto flotante.\n",
    "\n",
    "    Args:\n",
    "    - str_repeated_value (str): Cadena con números repetidos.\n",
    "\n",
    "    Returns:\n",
    "    - float: Valor numérico sin números repetidos\n",
    "    \"\"\"\n",
    "    str_sin_0 = re.sub(\"^0+\", '', str_repeated_value)\n",
    "    str_sin_comma = str_sin_0.replace(',','')\n",
    "    mitad = len(str_sin_comma) // 2\n",
    "    num = float(str_sin_comma[0:mitad]) if len(str_sin_comma) % 2 == 0 else 0.0\n",
    "    return num\n",
    "\n",
    "def extract_int_number(str_value: str) -> int:\n",
    "    \"\"\"\n",
    "    Extrae el número entero de una cadena.\n",
    "\n",
    "    Args:\n",
    "    - str_value (str): Cadena con un número entero.\n",
    "\n",
    "    Returns:\n",
    "    - int: Número entero extraido de la cadena.\n",
    "    \"\"\"\n",
    "    str_value_clean = re.findall(r'[\\d,\\.]*', str_value)[0]\n",
    "    str_sin_0 = re.sub(\"^0+\", '', str_value_clean)\n",
    "    str_sin_comma = str_sin_0.replace(',','')\n",
    "    return int(str_sin_comma)\n",
    "\n",
    "\n",
    "def remove_repeated_date(str_date_repeated: str) -> datetime:\n",
    "    \"\"\"\n",
    "    Elimina la repetición de fecha de una cadena y la convierte en un objeto de\n",
    "    fecha.\n",
    "\n",
    "    Args:\n",
    "    - str_date_repeated (str): Cadena con fecha repetida.\n",
    "\n",
    "    Returns:\n",
    "    - datetime: Objeto con la fecha sin repetición.\n",
    "    \"\"\"\n",
    "    return datetime.strptime(str_date_repeated[0:8],'%Y%m%d')\n",
    "\n",
    "def limpiar_area(area: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Limpia la cadena que contiene información sobre el área y devuelve dos\n",
    "    valores: kilómetros y millas.\n",
    "\n",
    "    Args:\n",
    "    - area (str): Cadena que contiene información sobre el área.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tupla con dos valores (kilómetros y millas) después de la\n",
    "    limpieza.\n",
    "    \"\"\"\n",
    "    str_en_partes = re.findall(r'[\\d,\\.]*', area)\n",
    "    str_en_partes.remove('2')   # Elimina el primer '2'\n",
    "    blancos = str_en_partes.count('')\n",
    "    for blanco in range(blancos):\n",
    "        str_en_partes.remove('')    # Elimina los elementos en blanco\n",
    "\n",
    "    km_float = remove_repeated_number(str_en_partes[0])\n",
    "    mi_float = float(str_en_partes[1].replace(',',''))\n",
    "    return km_float, mi_float\n",
    "\n",
    "df = pd.read_csv(\"csv/estados.csv\")\n",
    "df = df.drop(['Coat of arms'], axis=1)\n",
    "df.columns = ['estado', 'nombre_oficial', 'capital', 'ciudad_mas_grande',\n",
    "                'area', 'poblacion_2020', 'num_de_municipios', 'lugar',\n",
    "                'fecha_de_admision']\n",
    "\n",
    "df['lugar'] = df['lugar'].apply(lambda x: remove_repeated_number(x))\n",
    "df['poblacion_2020'] = df['poblacion_2020'].transform(remove_repeated_number)\n",
    "df['fecha_de_admision'] = df['fecha_de_admision'] \\\n",
    "                            .transform(remove_repeated_date)\n",
    "df['num_de_municipios'] = df['num_de_municipios'].transform(extract_int_number)\n",
    "\n",
    "areas = df['area'].transform(limpiar_area).to_list()\n",
    "df['area_km2'] = [a[0] for a in areas]\n",
    "df['area_mi'] = [a[1] for a in areas]\n",
    "df = df.drop(['area'], axis=1)\n",
    "\n",
    "print_tabulate(df)\n",
    "df.to_csv(\"csv/estados_limpio.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
